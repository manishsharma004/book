\section{Spark Streaming}\label{s:spark-streaming}

\FILENAME\

\subsection{Streaming Concepts}

Spark Streaming is one of the components extending from Core Spark.
Spark streaming provides a scalable fault tolerant system with high
throughput. For streaming data into spark, there are many libraries
like Kafka, Flume, Kinesis, etc.

\subsection{Simple Streaming Example}

In this section, we are going to focus on making a simple streaming
application using the network in your computer. Here we are going to
expose a particular port and from that port we are going to
continously stream data by user entries and the word count is being
calculated as the output.

First, create a Makefile

\begin{lstlisting}
  mkdir -p ~/cloudmesh/spark/examples/streaming
  cd ~/cloudmesh/spark/examples/streaming
  emacs Makefile
\end{lstlisting}

Then add the following content to Makefile. 
\begin{NOTE}
  Please add a tab when adding the corresponding command for a given instruction
  in Makefile. In pdf mode the tab is not clearly shown. 
\end{NOTE}
\begin{lstlisting}
  SPARKHOME = ${SPARK_HOME}
  run-streaming:
	${SPARKHOME}/bin/spark-submit streaming.py localhost 9999
\end{lstlisting}
      
Now we need to create file called streaming.py

\begin{lstlisting}
  emacs streaming.py
\end{lstlisting}

Then add the following content. 

\begin{lstlisting}
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Create a local StreamingContext with two working thread and batch interval of 1 second
sc = SparkContext("local[2]", "NetworkWordCount")

log4jLogger = sc._jvm.org.apache.log4j
LOGGER = log4jLogger.LogManager.getLogger(__name__)
LOGGER.info("Pyspark script logger initialized")

ssc = StreamingContext(sc, 1)


# Create a DStream that will connect to hostname:port, like localhost:9999
lines = ssc.socketTextStream("localhost", 9999)
# Split each line into words
words = lines.flatMap(lambda line: line.split(" "))
# Count each word in each batch
pairs = words.map(lambda word: (word, 1))
wordCounts = pairs.reduceByKey(lambda x, y: x + y)

# Print the first ten elements of each RDD generated in this DStream to the console
wordCounts.pprint()
ssc.start()             # Start the computation
ssc.awaitTermination(100)  # Wait for the computation to terminate

\end{lstlisting}

To run the code, we need to open up two terminals.

\verb|Terminal 1 :|

First use netstat to open up a port to start communication.

\begin{lstlisting}
  nc -lk 9999
\end{lstlisting}


\verb|Terminal 2 :|

Now run the Spark programme in the second terminal.

\begin{lstlisting}
  make run-streaming
\end{lstlisting}

In this terminal you can see an script running trying to read
the stream coming from the port 9999. You can enter texts in the
Terminal 1 and these texts will be tokenized and the word count is
calculated and the result is shown in the Terminal 2.

\subsection{Spark Streaming For Twitter Data}

In this section we are going to learn how to use Twitter data as the streaming data source and use Spark Streaming capabilities to process the data. As the first step you must install the python packages using pip.

\begin{lstlisting}
  pip install tweepy
\end{lstlisting}

First you need to create an account in Twitter Apps. 